{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": 3
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python_defaultSpec_1596705213208",
      "display_name": "Python 3.6.10 64-bit ('base': conda)"
    },
    "colab": {
      "name": "FeudalQLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yylmvuayMn6U",
        "colab_type": "text"
      },
      "source": [
        "# Feudal Q-Learning\n",
        "\n",
        "**Papers**\n",
        "* [Feudal Reinforcement Learning](http://www.cs.toronto.edu/~fritz/absps/dh93.pdf)\n",
        "* [Feudal Q Learning](https://pdfs.semanticscholar.org/f160/10cc4ca903b27b24bc3a3df0165c8161b8ff.pdf)\n",
        "\n",
        "In this notebook I'm exploring Feudal Reinforcement Learning. Feudal tries to mirror the hierachical aspect of a feudal fiefdom. Where we have managers, sub-manager at all levels of the hierarchy. And the manager can set tasks, reward or punish the sub-manager. In the end each manager tries to satisfy his super-manager.\n",
        "\n",
        "## About Feudal Q-Learning\n",
        "\n",
        "Feudal tries to map an environment in such a way that its performance drastically improves, compared to classic RL, when the Goal state changes and the environment remains the same. Instead of just having a Q matrix where each entry is a pair (s, a), with Feudal we have a stack of Q matrices. The stack is a group of levels, from Level-0 (the top one) to Level-N (bottom one). Each Level uses Q-Learning to learn its own envorinment. Each Level has its own set of states, the Level-N is just like the traditional RL where one state corresponds to an environment state, then as we go up in the stack the states are reduced by a factor of two.\n",
        "\n",
        "**Manager, sub-manager, super-manager**: If we consider a Level-1, then: Level-1 is the manager, Level-2 is the sub-manager, and Level-0 is the super-manager.\n",
        "\n",
        "\n",
        "The paper introduces the terms \"reward hiding\", and \"information hiding\" and they are very important to understand how the levels interact with each other: \n",
        " \n",
        "* **Information Hiding** - managers have no information about the means by which submanagers carry out their tasks. They do get to see the amount of reinforcement accrued from the world in the course of execution of their commands. Sub-managers also do not get any information about the tasks that their managers were set by their own managers. These restrictions can lead to sub-optimal ultimate paths, that is the price to be paid for the abstraction benefits of the hierarchy.\n",
        "* **Reward Hiding** - managers should reward their sub-managers for carrying out their bidding whether or not the managers were themselves rewarded by the supermanagers. In the early stages what is important is that the sub-managers perform according to the manager's intentions, even if the manager is wrong.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://d3i71xaburhd42.cloudfront.net/1678bd32846b1aded5b1e80a617170812e80f562/4-Figure1-1.png\" alt=\"The Stack\" width=\"400\"/></center>\n",
        "\n",
        "### States\n",
        "\n",
        "For each level (except zero) the state is compost by the environment state, i.e (x, y), and the corresponding level manager's action, resulting in \"(x, y, a)\" \n",
        "\n",
        "### Actions\n",
        "\n",
        "Like the paper the environment I used has 4 actions: N, S, E, W. The paper also introduces a Feudal specific action represented by the symbol `*`, this action means that the manager believes that the Goal is inside his current state. Level-0 only has one action, `*`, because only has one state. Beyond that, Levels from Level-1 to Level-(N-1) have the environment's 4 actions plus the `* `. And Level-N has the environment's 4 actions.\n",
        "\n",
        "## Doubts\n",
        "\n",
        "The paper never explains fully how the Level-0 works, the way I understand this Feudal is that each Level selects an action according to his own state. And that action will serve as a hint for the sub-manager. So, I don't understand how Level-0 can pick an action other than `*` since it only has one state. The only good reference to Level-0 is this one: \"Clearly only the `*` action is available at level 0, where there is just one state\". For that reason, and as I explained above, my Level-0 is just a \"dummy\" Q-Learning because it doesn't learn anything and only returns one action, `*`. I have tested both versions and the one I chose performs better.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQW-Ra3BMsBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!git clone https://github.com/MattChanTK/gym-maze.git && cd gym-maze && python setup.py install"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "cnGda3EPMn6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gym\n",
        "import gym_maze\n",
        "\n",
        "# Disable pygames video, used by gym_maze \n",
        "import os\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOlvkOyNMn6h",
        "colab_type": "text"
      },
      "source": [
        "## FeudalQLearning\n",
        "\n",
        "This is where we manage all the Feudal Levels. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCk00_vlMn6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeudalQlearning():\n",
        "    \n",
        "    def __init__(self, n_levels=4):\n",
        "        self.n_levels = n_levels\n",
        "        self.levels_stack = [NLevelQLearning(i) for i in range(n_levels)]  # 0: Level 0, 1: Level 1, etc\n",
        "\n",
        "    def load_env(self, actions_space, observation_space):\n",
        "        self.actions = [0]*self.n_levels\n",
        "        self.actions[0] = 4\n",
        "        self.states = []\n",
        "\n",
        "        for i in range(self.n_levels):\n",
        "            n_actions = actions_space if i == self.n_levels-1 else actions_space + 1\n",
        "            self.levels_stack[i].load_env(n_actions)\n",
        "\n",
        "    def choose_actions(self, state):\n",
        "        levels_states = self.get_levels_states(state)\n",
        "        all_actions = []\n",
        "        for i in range(self.n_levels):\n",
        "            all_actions.append(self.levels_stack[i].choose_action(levels_states[i]))\n",
        "        self.actions = all_actions \n",
        "        return all_actions\n",
        "\n",
        "    def learn(self, s, a, r, s_, done):\n",
        "        levels_curr_states = self.get_levels_states(s, )        \n",
        "        levels_next_states = self.get_levels_states(s_)\n",
        "\n",
        "        for i in range(self.n_levels):\n",
        "            reward = r\n",
        "            if i > 0:\n",
        "                reward = self.sub_manager_action_reward(a, i, r, s, s_)\n",
        "\n",
        "            self.levels_stack[i].learn(levels_curr_states[i], a[i], reward, levels_next_states[i], done)\n",
        "\n",
        "    def sub_manager_action_reward(self, actions, level, r, s, s_):\n",
        "        if actions[level-1] == 4: # special * action\n",
        "            return r\n",
        "        return 0 if actions[level-1] == actions[level] and not s == s_  else -1\n",
        "\n",
        "    def get_levels_states(self, state):\n",
        "        states = [(state[0], state[1], self.actions[self.n_levels-2])]\n",
        "        for i in range(self.n_levels-2, -1 , -1):\n",
        "            states.append((math.ceil(states[-1][0]/2), math.ceil(states[-1][1]/2), self.actions[i-1]))\n",
        "        states.reverse()\n",
        "        states = [str(i) for i in states]\n",
        "        return states\n",
        "    \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92CQQ1Q5Mn6r",
        "colab_type": "text"
      },
      "source": [
        "## NLevelQLearning \n",
        "\n",
        "### Q Table\n",
        "\n",
        "I represent the Q table with a pandas Dataframe. These method is very memory intensive and I use the Dataframe in an atempt to save some space, it also allows me to use the state representation, `(x, y, manager_action)`, as the index for an easy access.\n",
        "\n",
        "### Q-Learning update  \n",
        "$$\n",
        "Q(s_{t},a_{t})=Q(s_{t},a_{t})+\\alpha*(r+\\gamma*max_{a}Q(s_{t+1},a)-Q(s_{t}, a_{t}))\n",
        "$$\n",
        "\n",
        "\n",
        "**Learning Rate:** $\\alpha_{k}=\\frac{\\alpha_{0}*\\tau}{\\tau+n}$\n",
        "\n",
        "**Exploration/Exploitation:**\n",
        "\n",
        "Just like the paper I used the softmax function:\n",
        "\n",
        "$$\n",
        "P(a) = \\frac{e^{\\frac{Q(s, a)}{T_{l}}}}{\\sum_{v\\in A_{i}}e^{\\frac{Q(s, v)}{T_{l}}}}\n",
        "$$\n",
        "where:\n",
        "$$\n",
        "T_{0} = T_{MAX}\\\\\n",
        "T_{l+1} = T_{MIN}+\\beta*(T_{l}-T_{MIN})\n",
        "$$\n",
        "\n",
        "Because this is a simple exeperiment I didn't consider Goal changes and I didn't spend much of my time testing different parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNp8haW9Mn6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NLevelQLearning():\n",
        "\n",
        "    def __init__(self, n_level):\n",
        "        self.n_level = n_level\n",
        "        self.alpha = 0.5\n",
        "        self.gamma = 0.9\n",
        "        self.epsilon = 0.5\n",
        "        self.tau = 5000\n",
        "        self.T_min = 0.2\n",
        "        self.T_max = 10000\n",
        "        self.T_l = self.T_max\n",
        "        self.beta = 0.9995\n",
        "        self.n = 0\n",
        "\n",
        "    def load_env(self, n_actions):\n",
        "        self.actions = list(range(n_actions))\n",
        "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
        "        self.curr_state = \"\"\n",
        "        self.curr_action = 0\n",
        "\n",
        "    def choose_action(self, s):\n",
        "        if self.n_level == 0:\n",
        "            return 4\n",
        "\n",
        "        if not self.state_exists(s):\n",
        "            self.add_state(s)\n",
        "        soft_dist = self._softmax(s)\n",
        "    \n",
        "        action = np.random.choice(self.actions, p=soft_dist)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def learn(self, s, a, r, s_, done):\n",
        "        if self.n_level == 0:\n",
        "            return   \n",
        "\n",
        "        if not self.state_exists(s):\n",
        "            self.add_state(s)\n",
        "        if not self.state_exists(s_):\n",
        "            self.add_state(s_)\n",
        "        \n",
        "        self.n += 1\n",
        "        alpha_k = (self.alpha * self.tau) / (self.tau + self.n)\n",
        "        \n",
        "        td_target = r + self.gamma * self.q_table.loc[s_].max()\n",
        "\n",
        "        self.q_table.loc[s, a] = self.q_table.loc[s, a] + alpha_k * (td_target - self.q_table.loc[s, a]) \n",
        "\n",
        "\n",
        "    def state_exists(self, state):\n",
        "        return state in self.q_table.index\n",
        "\n",
        "    def add_state(self, state_name):\n",
        "        self.q_table = self.q_table.append(\n",
        "            pd.Series(\n",
        "                [self.q_start_value()] * len(self.actions),\n",
        "                index=self.q_table.columns,\n",
        "                name=state_name,\n",
        "                dtype=np.float64\n",
        "                )\n",
        "            )\n",
        "    \n",
        "    def q_start_value(self):\n",
        "        return 1\n",
        "\n",
        "    def _softmax(self, s):\n",
        "      dist = []\n",
        "      values = []\n",
        "      denominator = 0\n",
        "\n",
        "      for a in range(len(self.actions)):\n",
        "        val = np.exp(self.q_table.loc[s, a]/self.T_l)\n",
        "        values.append(val)\n",
        "        denominator += val\n",
        "      \n",
        "      for val in values:\n",
        "        dist.append(val/denominator)\n",
        "      self.T_l = self.T_min + self.beta * (self.T_l - self.T_min)\n",
        "      return np.array(dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8s3hqHPMn6x",
        "colab_type": "text"
      },
      "source": [
        "## Train\n",
        "\n",
        "Nothing major here just the standard environment iteraction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hu8Ij-lMn6z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def wrap_state(state):\n",
        "    return (int(state[0]), int(state[1]))\n",
        "\n",
        "def train():\n",
        "    NUM_EPISODES = 5000\n",
        "    BATCH_SIZE = 20\n",
        "\n",
        "    env = gym.make(\"maze-sample-5x5-v0\")\n",
        "    env_action = {0: 'N', 1: 'S', 2: 'E', 3: 'W'}\n",
        "\n",
        "    # Agent\n",
        "    agent = FeudalQlearning()\n",
        "    agent.load_env(env.action_space.n, 0)\n",
        "\n",
        "    n_steps_arr = np.empty(0, dtype=int)\n",
        "\n",
        "    batch_avg = 0\n",
        "    batch_n = 0\n",
        "\n",
        "    for episode in range(NUM_EPISODES):\n",
        "\n",
        "        # Reset the environment\n",
        "        obs = env.reset()\n",
        "\n",
        "        total_reward = 0\n",
        "        n_steps = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            s = wrap_state(obs)\n",
        "\n",
        "            # Select an action\n",
        "            actions = agent.choose_actions(s)\n",
        "            agent_action = actions[-1]\n",
        "\n",
        "            # Execute the action\n",
        "            obs, reward, done, _ = env.step(env_action[agent_action])\n",
        "            s_ = wrap_state(obs)\n",
        "\n",
        "            total_reward += reward\n",
        "            n_steps += 1\n",
        "\n",
        "            agent.learn(s, actions, reward, s_, done)\n",
        "        \n",
        "        batch_n += 1\n",
        "        batch_avg += n_steps/BATCH_SIZE\n",
        "\n",
        "        if batch_n % BATCH_SIZE == 0:\n",
        "          print(\"Steps: \" + str(batch_avg))\n",
        "          n_steps_arr = np.append(n_steps_arr, batch_avg)\n",
        "          batch_avg = 0\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    x = np.arange(1,(NUM_EPISODES/BATCH_SIZE)+1)\n",
        "    plt.plot(x,n_steps_arr)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "bwi8m1MGMn68",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6e2a5923-b1db-485a-f284-9aa4805a70ed"
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 615.9\n",
            "Steps: 343.89999999999986\n",
            "Steps: 134.0\n",
            "Steps: 92.79999999999998\n",
            "Steps: 74.49999999999999\n",
            "Steps: 90.05000000000003\n",
            "Steps: 95.7\n",
            "Steps: 95.0\n",
            "Steps: 81.85000000000001\n",
            "Steps: 106.24999999999999\n",
            "Steps: 87.9\n",
            "Steps: 79.15\n",
            "Steps: 83.55\n",
            "Steps: 81.19999999999999\n",
            "Steps: 75.89999999999999\n",
            "Steps: 82.30000000000001\n",
            "Steps: 63.000000000000014\n",
            "Steps: 79.69999999999999\n",
            "Steps: 61.199999999999996\n",
            "Steps: 71.84999999999998\n",
            "Steps: 70.85\n",
            "Steps: 58.90000000000001\n",
            "Steps: 88.65\n",
            "Steps: 54.949999999999996\n",
            "Steps: 56.45\n",
            "Steps: 70.2\n",
            "Steps: 79.29999999999997\n",
            "Steps: 60.400000000000006\n",
            "Steps: 51.35\n",
            "Steps: 64.64999999999999\n",
            "Steps: 55.04999999999999\n",
            "Steps: 66.94999999999999\n",
            "Steps: 68.14999999999999\n",
            "Steps: 46.69999999999998\n",
            "Steps: 69.45\n",
            "Steps: 55.849999999999994\n",
            "Steps: 45.25\n",
            "Steps: 58.24999999999999\n",
            "Steps: 53.15\n",
            "Steps: 61.24999999999999\n",
            "Steps: 61.949999999999996\n",
            "Steps: 68.64999999999999\n",
            "Steps: 57.25\n",
            "Steps: 37.1\n",
            "Steps: 58.150000000000006\n",
            "Steps: 57.5\n",
            "Steps: 62.949999999999996\n",
            "Steps: 52.85\n",
            "Steps: 47.300000000000004\n",
            "Steps: 53.050000000000004\n",
            "Steps: 46.20000000000001\n",
            "Steps: 57.25000000000001\n",
            "Steps: 54.199999999999996\n",
            "Steps: 47.800000000000004\n",
            "Steps: 44.550000000000004\n",
            "Steps: 43.60000000000001\n",
            "Steps: 56.25000000000001\n",
            "Steps: 46.9\n",
            "Steps: 44.95000000000001\n",
            "Steps: 38.550000000000004\n",
            "Steps: 46.800000000000004\n",
            "Steps: 48.65000000000001\n",
            "Steps: 42.25\n",
            "Steps: 44.85\n",
            "Steps: 62.099999999999994\n",
            "Steps: 46.45000000000001\n",
            "Steps: 55.85\n",
            "Steps: 43.6\n",
            "Steps: 41.0\n",
            "Steps: 45.900000000000006\n",
            "Steps: 46.95\n",
            "Steps: 51.45\n",
            "Steps: 51.099999999999994\n",
            "Steps: 60.7\n",
            "Steps: 44.75000000000001\n",
            "Steps: 42.35\n",
            "Steps: 58.85000000000001\n",
            "Steps: 47.5\n",
            "Steps: 43.00000000000001\n",
            "Steps: 49.60000000000001\n",
            "Steps: 47.99999999999999\n",
            "Steps: 55.349999999999994\n",
            "Steps: 33.5\n",
            "Steps: 47.050000000000004\n",
            "Steps: 49.14999999999999\n",
            "Steps: 53.35\n",
            "Steps: 48.35\n",
            "Steps: 50.900000000000006\n",
            "Steps: 41.4\n",
            "Steps: 45.900000000000006\n",
            "Steps: 36.050000000000004\n",
            "Steps: 42.05\n",
            "Steps: 47.8\n",
            "Steps: 40.5\n",
            "Steps: 43.05\n",
            "Steps: 41.699999999999996\n",
            "Steps: 49.050000000000004\n",
            "Steps: 45.65\n",
            "Steps: 38.95\n",
            "Steps: 60.550000000000004\n",
            "Steps: 38.0\n",
            "Steps: 58.15\n",
            "Steps: 48.15\n",
            "Steps: 36.2\n",
            "Steps: 46.00000000000001\n",
            "Steps: 48.050000000000004\n",
            "Steps: 48.550000000000004\n",
            "Steps: 39.25\n",
            "Steps: 48.900000000000006\n",
            "Steps: 50.85000000000001\n",
            "Steps: 46.75\n",
            "Steps: 49.199999999999996\n",
            "Steps: 49.949999999999996\n",
            "Steps: 48.94999999999999\n",
            "Steps: 45.800000000000004\n",
            "Steps: 34.65\n",
            "Steps: 55.65\n",
            "Steps: 50.500000000000014\n",
            "Steps: 37.8\n",
            "Steps: 47.800000000000004\n",
            "Steps: 39.14999999999999\n",
            "Steps: 52.25000000000001\n",
            "Steps: 42.15\n",
            "Steps: 38.400000000000006\n",
            "Steps: 35.5\n",
            "Steps: 37.90000000000001\n",
            "Steps: 48.55\n",
            "Steps: 39.6\n",
            "Steps: 54.400000000000006\n",
            "Steps: 40.4\n",
            "Steps: 52.25\n",
            "Steps: 40.45\n",
            "Steps: 37.65\n",
            "Steps: 39.0\n",
            "Steps: 37.8\n",
            "Steps: 44.2\n",
            "Steps: 38.300000000000004\n",
            "Steps: 43.400000000000006\n",
            "Steps: 44.20000000000001\n",
            "Steps: 43.75\n",
            "Steps: 62.150000000000006\n",
            "Steps: 37.449999999999996\n",
            "Steps: 40.449999999999996\n",
            "Steps: 45.849999999999994\n",
            "Steps: 47.400000000000006\n",
            "Steps: 43.849999999999994\n",
            "Steps: 38.3\n",
            "Steps: 45.849999999999994\n",
            "Steps: 52.55\n",
            "Steps: 38.150000000000006\n",
            "Steps: 43.65\n",
            "Steps: 56.150000000000006\n",
            "Steps: 42.75\n",
            "Steps: 44.199999999999996\n",
            "Steps: 44.749999999999986\n",
            "Steps: 44.9\n",
            "Steps: 59.00000000000001\n",
            "Steps: 43.95\n",
            "Steps: 39.25000000000001\n",
            "Steps: 49.05\n",
            "Steps: 39.25000000000001\n",
            "Steps: 41.300000000000004\n",
            "Steps: 42.95\n",
            "Steps: 50.1\n",
            "Steps: 36.499999999999986\n",
            "Steps: 36.9\n",
            "Steps: 55.3\n",
            "Steps: 44.1\n",
            "Steps: 41.35000000000001\n",
            "Steps: 50.45\n",
            "Steps: 51.2\n",
            "Steps: 45.449999999999996\n",
            "Steps: 44.99999999999999\n",
            "Steps: 46.75\n",
            "Steps: 39.25000000000001\n",
            "Steps: 36.05000000000001\n",
            "Steps: 43.75000000000001\n",
            "Steps: 40.35\n",
            "Steps: 33.800000000000004\n",
            "Steps: 38.5\n",
            "Steps: 47.89999999999999\n",
            "Steps: 46.15\n",
            "Steps: 48.5\n",
            "Steps: 43.85\n",
            "Steps: 45.050000000000004\n",
            "Steps: 33.60000000000001\n",
            "Steps: 41.2\n",
            "Steps: 47.65\n",
            "Steps: 45.00000000000001\n",
            "Steps: 40.74999999999999\n",
            "Steps: 42.35\n",
            "Steps: 45.95\n",
            "Steps: 47.55\n",
            "Steps: 51.050000000000004\n",
            "Steps: 37.65000000000001\n",
            "Steps: 45.35\n",
            "Steps: 39.05\n",
            "Steps: 42.8\n",
            "Steps: 34.150000000000006\n",
            "Steps: 38.05\n",
            "Steps: 35.65\n",
            "Steps: 52.65\n",
            "Steps: 35.949999999999996\n",
            "Steps: 48.300000000000004\n",
            "Steps: 35.800000000000004\n",
            "Steps: 49.150000000000006\n",
            "Steps: 33.89999999999999\n",
            "Steps: 42.900000000000006\n",
            "Steps: 49.80000000000001\n",
            "Steps: 43.85000000000001\n",
            "Steps: 40.199999999999996\n",
            "Steps: 51.70000000000001\n",
            "Steps: 34.45\n",
            "Steps: 36.29999999999999\n",
            "Steps: 48.4\n",
            "Steps: 38.05\n",
            "Steps: 57.8\n",
            "Steps: 40.400000000000006\n",
            "Steps: 44.7\n",
            "Steps: 37.85\n",
            "Steps: 47.84999999999999\n",
            "Steps: 38.050000000000004\n",
            "Steps: 40.64999999999999\n",
            "Steps: 36.45000000000001\n",
            "Steps: 38.35\n",
            "Steps: 44.550000000000004\n",
            "Steps: 35.1\n",
            "Steps: 42.050000000000004\n",
            "Steps: 44.05\n",
            "Steps: 41.099999999999994\n",
            "Steps: 39.300000000000004\n",
            "Steps: 37.7\n",
            "Steps: 39.95\n",
            "Steps: 33.75\n",
            "Steps: 41.74999999999999\n",
            "Steps: 50.449999999999996\n",
            "Steps: 42.25\n",
            "Steps: 47.75\n",
            "Steps: 36.8\n",
            "Steps: 34.75000000000001\n",
            "Steps: 38.199999999999996\n",
            "Steps: 36.45\n",
            "Steps: 42.10000000000001\n",
            "Steps: 42.2\n",
            "Steps: 42.099999999999994\n",
            "Steps: 37.449999999999996\n",
            "Steps: 47.75\n",
            "Steps: 44.29999999999999\n",
            "Steps: 47.999999999999986\n",
            "Steps: 46.75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHqiPQEzMn7E",
        "colab_type": "text"
      },
      "source": [
        "## Final Thoughts\n",
        "\n",
        "**Implementation:** For very big environments the Boltzmann exploration might not be enough, since it realies on the agent capability to reach all states multiple time with somewhat random actions. \n",
        "\n",
        "**Idea:** I really like the ideia, it is very interesting and all the work inspired by it is also very compelling, for example, FeudalNetworks. \n",
        "The search for an agent that can learn to perform tasks in a hierarchical way is very important in my opinion, I am not sure this aproach is the right way, but is certainly a start. \n",
        "\n"
      ]
    }
  ]
}